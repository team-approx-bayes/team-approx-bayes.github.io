---
title: Effective Variational Learning on GPT-2 
date: 2024-03-15
categories:
  - blog
author: Emtiyaz Khan
---

In our recent work, we present new results on training large networks (such as GPT-2) using variational (Bayesian) learning. Our variational approach improves performance over AdamW while having nearly identical runtime, but it yields good uncertainty estimates that improve many aspects of deep learning. These results are exciting because they go against the common belief that Bayesian methods are impractical at large scale because, no way, we can run them on GPT, right? Even when we can, we don't expect them to match the performance of SGD or Adam. Perhaps you have heard the argument that there might a trade-off between accuracy and uncertainty. We do not find any such problems, so this is personally very refreshing result for me. In what follows, I will summarize the main results, describe the method, and also give a brief history of how we got here.

**MAIN RESULTS**

The most exciting result for me is on GPT-2 where our new optimizer, called IVON, gives better performance than AdamW. We ran it on OpenWebText data which contains 49.2 billion tokens. We fit three GPT-2 model with parameters 773M, 255M, and 125M respectively. These are quite large data and models but IVON closely follows the trajectory of AdamW with a slightly
better performance in the end to get around 0.4-0.4 increase in perplexity. Some NLP researchers tell me this is significant and I believe them.

Add figure 1.

In terms of run-time, IVON is close but slightly slower because the parameters are sampled from a Gaussian posterior which is an additional overhead (more details on this later). The cost of posterior sampling is worth it because it makes IVON's weight-uncertainty (that is, variance estimates of the weights) much better than other deep-learning optimizers. For instance, on ImageNet, IVON not only gets better accuracy than both AdamW and SGD (around 1-2% better) but its predictions are also better calibrated. The cost of posterior sampling is negligible at ImageNet scale and the runtime of the IVON is almost identical to AdamW.

Add figure 2.

Thanks to posterior sampling, IVON does not suffer from overfitting unlike AdamW. It is well-known for image classification that AdamW, when run for a long time without early stopping, start to overfit. It also consistently underperforms compared to SGD. IVON does not have this problem and we see consistent improvement over SGD and no overfitting like AdamW.

Add figure 3.

There are additional benefits of IVON which I will describe later. These include
<ol>
<li> It gives better predictive uncertainty than alternatives, such as, MC-dropout and SWAG</li>
<li> It works well for finetuning LLMs and reduces the cost of model-merging</li>
<li> It can be used to faithfully predict generalization which is useful for diagnostics and early stopping</li>
<li>It is useful to understand sensitivity to data which is often challenging at large-scale due to ill-conditioning.</li>
</ol>

Before these, let me briefly describe the IVON algorithm first.

**THE IVON ALGORITHM**

The IVON algorithm is derived using the Bayesian Learning rule of Khan and Rue and finds a Gaussian posterior-approximation over network weights by optimizing a variational (Bayesian) objective. Unlike AdamW, which returns a trained weight vector, IVON additional yields a variance vector which corresponds to the posterior variance of the Gaussian posterior. 

It is easier to understand the optimizer by directly comparing it to AdamW, as shown in this (not so precise) pseudo-code where the differences are highlighted in red.

Add figure 4.

There are following differences
<ol>
<li>Unlike AdamW, which returns one vector $\theta$, IVON returns two vectors: the mean $m$ and variance $\sigma^2$; see line 4-5. The mean $m$ is a direct replacement of $\theta$ and can be used in the same fashion to perform predictino, but the variance vector is additional. The variance vector provides an uncertainty/confidence estimate: larger entries in $\sigma$ imply that those entries in $m$ can change a lot, for example, if training conditions are changed (for example, data is perturbed).</li>
<li>To estimate $\sigma$ faithfully, IVON perturbs the location where gradients are computed. This is done in line 1 where $\theta$ is sampled from a Gaussian (or alternative $\theta = m + \sigma\cdot \epsilon$ where $\epsilon$ is a random noise from standard Gaussian. This is where the additional cost in IVON comes from, but this is a crucial step to get better $\sigma$.</li>
<li>Instead of using square of minibatch gradient $\hat{g}^2$, IVON computed a Monte-Carlo estimate of the Hessian. This is shown in line 2 where $\hat{g}$ is multiplied by $(\theta-m)/\sigma^2$. This is a reparameterization trick and gives an unbiased estimate of the Hessian. Unlike AdamW, IVON estimates an unbiased estimate of second-order information, which is subsequently used to compute $\sigma$ (in the last line)</li>
<li>In line 3, a correction term is added which ensures that the learning-rate adaptation vector $h$ is always positive. The cost of this step is minor but it makes the step stable and avoids issues for nonconvexity which can make the Hessian negative. The correction is derived using Riemannian gradient descent in the original work of Lin et al., but we will not go into the details of this. This step is not needed if we find $h$ never becomes negative in practice.</li>
<li>Finally, line 4 is almost identical to Adam but there is no square-root over $h$. Unlike Adam, IVON takes a Newton-like step.</li>
</ol>
A detailed pseudo-code containing momentum and debiasing steps is given in the paper. Many practical tricks are also discussed which are essential to make IVON work at large scale. To state them briefly, we have the following tricks: the $\delta$ parameter is set to values commonly used in AdamW; no debiasing for $h$ is used; never initialize $h$ to 0; clip the gradients; use similar learning rate schedule to Adam, etc.

Ultimately, we get an optimizer that can be used as a direct drop-in replacement for Adam. For example, in pytorch, we can simply add the following two lines.

Add figure 5.

The code is easily modifiable to include multiple GPUs, multiple MC samples, and also gradient and Hessian accumulation steps.

**HOW TO USE IVON**

The straightforward method is to use the $m$ vector of IVON like the trained weights $\theta$ returned by AdamW. For instance, we can simply do the following:

Add figure 6.

A better way is to use ``posterior averaging''. Essentially, we sample multiple $\theta$ from a Gaussian, predict using each of those, and then average the prediction to get the mean and variance. Below, is an example code for this.

Add figure 7.

The second way gives us a way to compute confidence in the prediction. See this 1-D regression example in a Collab notebook, where IVON's posterior averaging gives less confident predictions in regions where there is no data. This is indicated by the higher spread of gray regions around the average prediction (shown with black line). One could use the softmax output from Adam to do the same thing but it does not give satisfactory uncertainty even for this simple
1-D example.

Add figure 8.

A few such use cases are given in the github repo. Posterior averaging can be beneficial to get better performance, as shown in the figure below. The dashed line is obtained by using just the mean (the first method above) while the solid line is obtained by using posterior averaging. As the number of Monte-Carlo samples are increased both the Accuracy and Negative Log-Likelihood (NLL) improve.

Add figure 9.

More samples give better estimate of the predictive probabilities, which do not affect accuracy much, but yield much better NLL. This is important when making risky decisions based on the model's predictions. For instance, if a model predicts that the patient has cancer or not, it matters if the probability of that decision is 0.90 or 0.51. Decisions based on prob >0.5 are the same in both cases, but the decision only makes sense for 0.9. Posterior averaging can be useful
for such cases.

**OTHER RESULTS: OOD DETECTION**


We have a result that clearly demonstrates that the uncertainty estimates are of reasonable quality. Essentially, we train on one dataset (call it in-domain) and try to predict another dataset that we believe to be different from the dataset (call it out-of-domain or OOD). We expect the predictions to be generally be more confident on in-domain dataset (smaller predictive entropy) while be less confident on out-of-domain dataset (larger predictive entropy). 

The figure shows histogram of predictive entropies for a model trained on CIFAR-10 and tested for OOD on Flowers102. Let us look at SGD first: the histograms are skewed towards lower values no matter which dataset it is (red-ish colors are in-domain and gray-ish colors are OOD). Compare this to IVON, which has a high peak (indicated by darker red color) at lower values for in-domain and generally assigns much higher values to OOD data (gray histogram is spread out to higher
values). Overall, the difference between the OOD and in-domain histograms is more pronounced with IVON.

Add figure 10.

Other methods are somehwere in between, with BBB being the worst. For both SWAG and MD-Dropout the peak for in-domain is not as high. The results are not conclusive to outright reject these two methods but with IVON we are able to get reasonable results while directly optimizing the variational bounds. The result supports our hypothesis that variational learning can be effective at large scale.

**OTHER RESULTS: SECOND ORDER INFORMATION**

Another attractive feature of IVON is that it yields, with no significant overhead, a sensible of the second-order information $h$. This is essentially the inverse of $\sigma^2$ because $\sigma^2 = 1/(h+\delta)$. An estimate of $h$ is available throughout training and we can use it for knowledge transfer and understanding. I am not aware of any other second-order optimizer that is as effective as IVON for deep learning. IVON is unique in that way.

We show a use for model-merging in Large Language Models (LLM) where a recent practice is to merge many fine-tune models by as imple arithmetic on their parameters.
For example, given two fine-tuned models $\theta_1$ and $\theta_2$ on an LLM $\theta_{LLM}$, we can simply do the following,

$$
\theta_{Merged} = \theta_{LLM} + (\theta_1 - \theta_{LLM}) + (\theta_2 - \theta_{LLM}).
$$

This is called task-arithmetic, but a better way is to rescale these estimate by Hessian vectors (here, we will use $h$ to denote $h+\delta$),

$$
\theta_{Merged} = \theta_{LLM} + \frac{h_1}{h_1+h_2} \cdot (\theta_1 - \theta_{LLM}) + \frac{h_2}{h_1+h_2} \cdot (\theta_2 - \theta_{LLM}).
$$

It is not hard to imagine why this is better. For example, the model with a very small curvature (lower $h$) should get a lower weight. In other words, the model with higher uncertainty (higher $\sigma$ or equivalent lower $h$) is less reliable and should be discounted. The second-order information helps us to add this additional information, which also leads to a better model-merging strategy as shown by Daheim et al. 2024.

When training with SGD or AdamW, we need to make additional effort to compute $h$. Often, an additional pass through the data is requierd. Often, for very large models, the estimate can be <i>ill-conditioned</i>, that is, it can have very small entries. IVON does not have this problem where we have an unbiased estimator of Hessian, coupled with a moving average, a weight-decay parameter $\delta$, and a correction term that ensured that $h$ remains positive throughout.
The result below for model-merging shows that IVON can match the performance of other Hessian techniques while incurring no computational overhead at all (SG is a method that computes square-of-gradients after training using SGD or AdamW type of methods).

Add figure 11.

I will finish with a last result that is really exciting for me where we use $h$ to understand sensitivity of the model to training examples. Essentially, we can estimate how much a model would change if we perturb training examples (for example, add or remove a subset of examples). We have recently done some work on using the Bayesian learning rule to extend sensitivity measures such as influence functions. There we show that, with IVON, we are able to estimate such measures during training and without a significant overhead.

The calculations are very simple: the sensitivity is proportional to the product of prediction error and prediction variance, 

$$
\text{sensitivity of an example} \propto \text{prediction error} \times \text{prediction variance}
$$

I have ignored the constants here for simplicity. Both of the quantities are readily available from IVON's code, making estimation of sensitivity very easy.

Below are some results where we show high and low sensitivity examples on ImageNet. Clearly, we see that low sensitivity examples conform to the category they are assigned, while high sensitivity examples are rather rebelious cases.

Add figure 12.

These results clearly demonstrate that second-order information is useful and that there is a clear benefit to shift from SGD style methods to Newton-like methods. In my opinion, the advantage of second-order methods is not to improve rate of convergence. It is unlikely for this to be true in general for deep learning. Even if these methods do not improve convergence rates, there are plenty of cases where second-order information can be extremely beneficial. To believe this, it
helps to think like a Bayesian. It is clear from IVON's code that posterior variance and second-order information are tightly connected. Therefore, any second-order method that improves Hessian estimation should also improve uncertainty estimates within a Bayesian framework.

**A BRIEF HISTORY**

It has been a long journey for us to get here. I started this line of work back in 2014 and published the first work on the Bayesian learning rule in 2017. The paper, although a personal breakthrough for me, focused on a narrow setting of variational inference and was difficult to appreciate for a broader audience. At the time, we wanted to build a codebase that will make the result more accessible to general machine-learning researchers. Fortunately or unfortunately, we
decided to focus on deep learning instead and wrote a series of papers, specializing on Gaussian mean-field. These results were quite promising: in our first paper in 2018, we were able to show similarities to Adam; in 2019, we were able to match the performance on ImageNet; and in 2020, we solved the problem of difficult Hessian computation and negative hessian estimates.

It was clear to me by this time that the algorithm should work, but I could not find a group of people who will push for a really good implementation that can be a drop0in replacement for SGD, Adam etc. In 2021, Thomas and colleagues did a simple implementation that won the NeurIPS 2021 challenge on Approximate Inference. This gave a good boost to the group to make some effort in writing a large-scale version.

We could have probably written a paper back then hastily, but we took almost 2 more years to make sure we do this well. Essentially, we had to find tricks that will work for most popular benchmarks and convince ourselves that this is really a good optimizer that gives us a reasonable posterior distribution as well as an estimate of the Hessian. Kudos to the team whoe made this a reality now!

We find overwhelming evidence that IVON is a good optimizer, at least as good as any other optimizer that exists. For us, this is an important milestone because, we believe that, IVON enables us to tackle the problem of knowledge understanding, transfer and collection. In coming years, we hope to have many new advances working towards an AI that can do continual lifelong learning just like we do. We hope that the community shares our enthusiasm and will also benefit from our work.

