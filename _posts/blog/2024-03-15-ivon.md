---
title: Effective Variational Learning with IVON on GPT-2 
date: 2024-03-15
categories:
  - blog
author: Emtiyaz Khan
---

Bayesian methods are considered impractical at large scale, and even when we are able to make them work for large deep networks (say using approximations), we don't expect them to match the performance of optimizers such as SGD or Adam. In [our recent work](https://arxiv.org/abs/2402.17641), we present results against this belief and show effectiveness of variational learning (called IVON) for large deep networks such as GPT-2, obtaining consistent improvement over AdamW with nearly identical runtime. These results are significant because they are obtained by <i>directly</i> optimizing the variational (Bayesian) objective. This is unlike methods like MC-dropout or SWAG which do not directly optimize such objectives, even though they have variational interpretations.
In this blog, I describe some major
point of the paper.
<ol>
<li><a href="#main-results">Main results</a></li>
<li><a href="#the-ivon-algorithm">IVON</a> as a <a href="#how-to-use-ivon">drop-in replacement of AdamW</a></li>
<li>Results on <a href="#other-results-ood-detection">OOD detection</a> and <a href="#other-results-second-order-information">2nd-order information</a></li>
<li> <a href="#a-brief-history">A brief history</a>.</li>
</ol>
For practitioner interested in using IVON reading Item 2 is sufficient.

Paper: [Variational Learning is Effective for Large Deep Networks](https://arxiv.org/abs/2402.17641)
<br>
Code: [https://github.com/team-approx-bayes/ivon](https://github.com/team-approx-bayes/ivon)

## MAIN RESULTS

The most exciting result is for GPT-2. We ran IVON on OpenWebText data (49.2 billion tokens) to fit three GPT-2 models with parameters sizes of 773M, 355M, and 125M respectively. These are quite large data and models but IVON closely follows the trajectory of AdamW with a slightly better performance in the end to get around 0.2-0.4 decrease in perplexity. Some NLP researchers tell me this is significant and I believe them.

<div style="text-align: center;">
<img src="/assets/images/ivonblog/gpt2.png" alt="IVON improves over AdamW on GPT-2" style="width:100%;max-width:400px">
</div>

For GPT-2, IVON's slightly lagging behind AdamW in terms of runtime which is due to posterior sampling (more on this later). However, the overhead due to sampling is worth it because it makes IVON's weight-uncertainty (that is, variance estimates of the weights) much better than AdamW. For instance, on ImageNet, IVON not only gets better accuracy than both AdamW and SGD (around 1-2%) but its predictions are also better calibrated. The cost of posterior sampling is negligible at ImageNet scale and the runtime of IVON is almost identical to AdamW.

<div style="text-align: center;"> 
<img src="/assets/images/ivonblog/imagenet.png" alt="IVON improves over AdamW on ImageNet" style="width:100%;max-width:400px"/>
<img src="/assets/images/ivonblog/imagenet.png" alt="IVON improves over AdamW on ImageNet" style="width:100%;max-width:400px"/>
</div>
<br>
 
An advantage of posterior sampling is that, unlike AdamW, IVON does not suffer from overfitting. It is well-known for image classification that AdamW, when run for a long time without early stopping, starts to overfit. It also consistently underperforms compared to SGD. IVON does not have this problem and we see consistent improvement over SGD and no overfitting like AdamW.

<div style="text-align: center;"> 
<img src="/assets/images/ivonblog/table_image.png" alt="When training for many epochs, IVON does not overfit unlike AdamW" style="width:100%;max-width:700px"/>
</div>
<br>

There are additional benefits of IVON, some of which I will describe later (see <a href="#other-results-ood-detection">OOD detection</a> and <a href="#other-results-second-order-information">2nd-order information</a>). These include
<ol>
<li> better predictive uncertainty than alternatives, such as, MC-dropout and SWAG.</li>
<li> its use for finetuning LLMs and to reduce the cost of model-merging.</li>
<li> faithful prediction of generalization for diagnostics and early stopping.</li>
<li> its use for understanding sensitivity to data which is often challenging at large-scale due to ill-conditioning.</li>
</ol>

Before these, let me briefly describe the IVON algorithm first.

## THE IVON ALGORITHM

The IVON algorithm is derived using the [Bayesian Learning rule](https://arxiv.org/abs/2107.04562) of Khan and Rue and finds a Gaussian posterior-approximation over network weights by optimizing a variational (Bayesian) objective. The easiest way to understand IVON by directly comparing it to Adam or RMSprop, as shown in this (not so precise) pseudo-code where the differences are highlighted in red.

<div style="text-align: center;"> 
<img src="/assets/images/ivonblog/pseudocode.png" alt="IVON vs RMSprop" style="width:100%;max-width:700px"/>
</div>
<br>

There are the following differences:
<ol>
<li>Unlike Adam, which returns one vector $\theta$, IVON returns two vectors: the mean $m$ and variance $\sigma^2$; see line 4-5. The mean $m$ plays the same role as $\theta$ in AdamW to perform predictions. The variance vector $\sigma$ provides an uncertainty/confidence estimate: larger entries imply that the corresponding entries in $m$ can change a lot, for example, when data of training conditions are perturbed.</li>
<li>To estimate $\sigma$ faithfully, IVON perturbs the location where gradients are computed. This is done in line 1 where $\theta$ is sampled from a Gaussian. This is where the additional cost in IVON comes from, but it is crucial to get better $\sigma$.</li>
<li>Instead of using square of minibatch gradient $\hat{g}^2$, IVON computes a Monte-Carlo estimate of the Hessian. This is shown in line 2 where $\hat{g}$ is multiplied by $(\theta-m)/\sigma^2$. This is simply the reparameterization trick and gives an unbiased estimate of the Hessian. You can think of this a perterbation to $\hat{g}$ to get a sense of the curvature; the perturbation $(\theta-m)/\sigma$ can be seen a sample from a standard Gaussian. Unlike Adam, IVON estimates an unbiased estimate of second-order information, which is subsequently used to compute $\sigma$ (in line 5)</li>
<li>In line 3, a correction term is added to ensure that the learning-rate adaptation vector $h$ is never negative. The cost of this step is minor but it improves stability of IVON by avoiding negative Hessians for nonconvex problems. The correction is derived using Riemannian gradient descent in the original work of <a href="https://arxiv.org/abs/2002.10060">Lin et al. 2020</a>, but I will not go into details here. This step is not needed if we find $h$ never becomes negative in practice.</li>
<li>Finally, line 4 is almost identical to Adam but there is no square-root over $h$. Unlike Adam, IVON takes a Newton-like step.</li>
</ol>
A detailed pseudo-code containing momentum and debiasing steps is given in the paper. Many practical tricks are also discussed which are essential to make IVON work at large scale. To state them briefly, we have the following tricks: the $\delta$ parameter is set to values commonly used in SGD/Adam; no debiasing for $h$ is used; never initialize $h$ to 0; clip the gradients; use similar learning rate schedule to Adam, etc.

Ultimately, we get an optimizer that can be used as a direct drop-in replacement for Adam. For example, in PyTorch, we can simply add the following highlighted lines to switch from Adam to IVON.

```diff
import torch
+import ivon

train_loader = torch.utils.data.DataLoader(train_dataset) 
test_loader = torch.utils.data.DataLoader(test_dataset) 
model = MLP()

-optimizer = torch.optim.Adam(model.parameters())
+optimizer = ivon.IVON(model.parameters())

for X, y in train_loader:

+    for _ in range(train_samples):
+       with optimizer.sampled_params(train=True)
            optimizer.zero_grad()
            logit = model(X)
            loss = torch.nn.CrossEntropyLoss(logit, y)
            loss.backward()

    optimizer.step()
```

The code is easily modifiable to include multiple GPUs, multiple MC samples, and also gradient and Hessian accumulation steps. [We provide](https://github.com/team-approx-bayes/ivon) an implementation of IVON as a PyTorch optimizer.

IVON is simple to integrate into other frameworks, as testified
by one of our collaborators :) 
<div style="text-align: center;"> 
<img src="/assets/images/ivonblog/fairseq.png" alt="Slack message" style="height: 150px;"/>
</div>
<br>

## HOW TO USE IVON

The straightforward method is to use the $m$ vector of IVON like the trained weights $\theta$ returned by AdamW. For instance, we can simply do the following:
```
for X, y in test_loader:
    logit = model(X)
    _, prediction = logit.max(1)
```
<br>

A better way is to use "posterior averaging". Essentially, we sample multiple $\theta$ from a Gaussian, predict using each of those, and then average the prediction to get the mean and variance. Below, is an example code for this.
```
for X, y in test_loader:
    sampled_probs = []
    for i in range(test_samples):
        with optimizer.sampled_params():
            sampled_logit = model(X)
            sampled_probs.append(F.softmax(sampled_logit, dim=1))
    prob = torch.mean(torch.stack(sampled_probs), dim=0)
    _, prediction = prob.max(1)
```
<br>

The second way gives us a way to compute confidence in the prediction. See this 1-D regression example in a [Colab notebook](https://colab.research.google.com/drive/1GcCCRfiZ6u7OwkYS46LGIAQKLnGL8Ae7?usp=sharing), where IVON's posterior averaging gives less confident predictions in regions where there is no data. This is indicated by the higher spread of gray regions around the average prediction (shown with black line). One could use the neural network's output uncertainty from Adam to do the same thing but it does not give satisfactory uncertainty even for this simple
1-D example, see the below plot on the right.

<div class="row" style="clear: both; display: table; margin-left:auto; margin-right: auto;">
  <div class="column" style="float: left; padding: 5px">
  <img src="/assets/images/ivonblog/1d_ivon.png" alt="" style="height: 200px;"/>
  </div>
  <div class="column" style="float: left; padding: 5px">
  <img src="/assets/images/ivonblog/1d_adamw.png" alt="" style="height: 200px;"/>
  </div>
  </div>
  <br>

A few such use cases are given in the [github repo](https://github.com/team-approx-bayes/ivon). Posterior averaging can be beneficial to get better performance, as shown in the figure below. The dashed line is obtained by using just the mean (the first method above) while the solid line is obtained by using posterior averaging. As the number of Monte-Carlo samples are increased both the Accuracy and Negative Log-Likelihood (NLL) improve.

<div style="text-align: center;"> 
<img src="/assets/images/ivonblog/mcsamples.png" alt="" style="height: 200px;"/>
</div>
<br>

More samples give better estimate of the predictive probabilities, which do not affect accuracy much, but yield much better NLL. This is important when making risky decisions based on the model's predictions. For instance, if a model predicts that the patient has cancer or not, it matters if the probability of that decision is 0.90 or 0.51. Decisions based on prob >0.5 are the same in both cases, but the decision only makes sense for 0.9. Posterior averaging can be useful
for such cases.

## OTHER RESULTS: OOD DETECTION


We have a result that clearly demonstrates that the uncertainty estimates are of reasonable quality. Essentially, we train on one dataset (call it in-domain) and try to predict another dataset that we believe to be different from the dataset (call it out-of-domain or OOD). We expect the predictions to be generally be more confident on in-domain dataset (smaller predictive entropy) while be less confident on out-of-domain dataset (larger predictive entropy). 

The figure shows histogram of predictive entropies for a model trained on CIFAR-10 and tested for OOD on Flowers102. Let us look at SGD first: the histograms are skewed towards lower values no matter which dataset it is (red-ish colors are in-domain and gray-ish colors are OOD). Compare this to IVON, which has a high peak (indicated by darker red color) at lower values for in-domain and generally assigns much higher values to OOD data (gray histogram is spread out to higher
values). Overall, the difference between the OOD and in-domain histograms is more pronounced with IVON.

<div style="text-align: center;"> 
<img src="/assets/images/ivonblog/oodflowers.png" alt="" style="height: 250px;"/>
</div>
<br>

Other methods are somehwere in between, with BBB being the worst. For both SWAG and MD-Dropout the peak for in-domain is not as high. The results are not conclusive to outright reject these two methods but with IVON we are able to get reasonable results while directly optimizing the variational bounds. The result supports our hypothesis that variational learning can be effective at large scale.

## OTHER RESULTS: SECOND ORDER INFORMATION

Another attractive feature of IVON is that it yields, with no significant overhead, a sensible of the second-order information $h$. This is essentially the inverse of $\sigma^2$ because $\sigma^2 = 1/(h+\delta)$. An estimate of $h$ is available throughout training and we can use it for knowledge transfer and understanding. I am not aware of any other second-order optimizer that is as effective as IVON for deep learning. IVON is unique in that way.

We show a use for model-merging in Large Language Models (LLM) where a recent practice is to merge many fine-tune models by as imple arithmetic on their parameters.
For example, given two fine-tuned models $\theta_1$ and $\theta_2$ on an LLM $\theta_{LLM}$, we can simply do the following,

$$
\theta_{Merged} = \theta_{LLM} + (\theta_1 - \theta_{LLM}) + (\theta_2 - \theta_{LLM}).
$$

This is called task-arithmetic, but a better way is to rescale these estimate by Hessian vectors (here, we will use $h$ to denote $h+\delta$),

$$
\theta_{Merged} = \theta_{LLM} + \frac{h_1}{h_1+h_2} \cdot (\theta_1 - \theta_{LLM}) + \frac{h_2}{h_1+h_2} \cdot (\theta_2 - \theta_{LLM}).
$$

It is not hard to imagine why this is better. For example, the model with a very small curvature (lower $h$) should get a lower weight. In other words, the model with higher uncertainty (higher $\sigma$ or equivalent lower $h$) is less reliable and should be discounted. The second-order information helps us to add this additional information, which also leads to a better model-merging strategy as shown by [Daheim et al. 2024](https://arxiv.org/abs/2310.12808).

When training with SGD or AdamW, we need to make additional effort to compute $h$. Often, an additional pass through the data is requierd. Often, for very large models, the estimate can be <i>ill-conditioned</i>, that is, it can have very small entries. IVON does not have this problem where we have an unbiased estimator of Hessian, coupled with a moving average, a weight-decay parameter $\delta$, and a correction term that ensured that $h$ remains positive throughout.
The result below for model-merging shows that IVON can match the performance of other Hessian techniques while incurring no computational overhead at all (SG is a method that computes square-of-gradients after training using SGD or AdamW type of methods).

<div style="text-align: center;"> 
<img src="/assets/images/ivonblog/merging.png" alt="" style="height: 75px;"/>
</div>
<br>

I will finish with a last result that is really exciting for me where we use $h$ to understand sensitivity of the model to training examples. Essentially, we can estimate how much a model would change if we perturb training examples (for example, add or remove a subset of examples). We have [recently done some work](https://arxiv.org/abs/2310.19273) on using the Bayesian learning rule to extend sensitivity measures such as influence functions, which we call the Memory Perturbation Equation. There we show that, with IVON, we are able to estimate such measures during training and without a significant overhead.

The calculations are very simple: the sensitivity is proportional to the product of prediction error and prediction variance, 

$$
\text{sensitivity of an example} \propto \text{prediction error} \times \text{prediction variance}
$$

I have ignored the constants here for simplicity. Both of the quantities are readily available from IVON's code, making estimation of sensitivity very easy.

Below are some results where we show high and low sensitivity examples on ImageNet. Clearly, we see that low sensitivity examples conform to the category they are assigned, while high sensitivity examples are rather rebelious cases.

<div style="text-align: center;"> 
<img src="/assets/images/ivonblog/mpe.png" alt="" style="height: 250px;"/>
</div>
<br>

These results clearly demonstrate that second-order information is useful and that there is a clear benefit to shift from SGD style methods to Newton-like methods. In my opinion, the advantage of second-order methods is not to improve rate of convergence. It is unlikely for this to be true in general for deep learning. Even if these methods do not improve convergence rates, there are plenty of cases where second-order information can be extremely beneficial. To believe this, it
helps to think like a Bayesian. It is clear from IVON's code that posterior variance and second-order information are tightly connected. Therefore, any second-order method that improves Hessian estimation should also improve uncertainty estimates within a Bayesian framework.

## A BRIEF HISTORY

It has been a long journey for us to get here. I started this line of work back in 2014 and published the [first work on the Bayesian learning rule](https://proceedings.mlr.press/v54/khan17a.html) in 2017. The paper, although a personal breakthrough for me, focused on a narrow setting of variational inference and was difficult to appreciate for a broader audience. At the time, we wanted to build a codebase that will make the result more accessible to general machine-learning researchers. Fortunately or unfortunately, we
decided to focus on deep learning instead and wrote a series of papers, specializing on Gaussian mean-field. These results were quite promising: in our [first paper](https://proceedings.mlr.press/v80/khan18a.html) we in 2018, we were able to show similarities to Adam; in 2019, we were able to [match the performance on ImageNet](https://arxiv.org/abs/1906.02506); and in 2020, [we solved](https://arxiv.org/abs/2002.10060) the problem of difficult Hessian computation and negative Hessian estimates.

It was clear to me by this time that the algorithm should work, but I could not find a group of people who will push for a really good implementation that can be a drop-in replacement for SGD, Adam and similar methods. In 2021, [Thomas](http://moellenh.github.io) and colleagues did a simple implementation that [won the NeurIPS 2021 challenge on Approximate Inference](https://izmailovpavel.github.io/neurips_bdl_competition/). See here for the [final presentation](https://www.youtube.com/watch?v=LQInlN5EU7E&t=16s) of the winning solution. This gave a good boost to the group to make some effort in writing a large-scale version.

We could have probably written a paper back then hastily, but we took almost 2 more years to make sure we do this well. Essentially, we had to find tricks that will work for most popular benchmarks and convince ourselves that this is really a good optimizer that gives us a reasonable posterior distribution as well as an estimate of the Hessian. Kudos to the team whoe made this a reality now!

We find overwhelming evidence that IVON is a good optimizer, at least as good as any other optimizer that exists. For us, this is an important milestone because, we believe that, IVON enables us to tackle the problem of knowledge understanding, transfer and collection. In coming years, we hope to have many new advances working towards an AI that can do continual lifelong learning just like we do. We hope that the community shares our enthusiasm and will also benefit from our work.

